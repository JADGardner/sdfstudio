{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading data batch <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading data batch \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the cwd to the root of the repo\n",
    "\n",
    "import os\n",
    "os.chdir(\"/users/jadg502/scratch/code/sdfstudio/\")\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from nerfstudio.configs import base_config as cfg\n",
    "from nerfstudio.configs.method_configs import method_configs\n",
    "from nerfstudio.data.dataparsers.nerfosr_dataparser import NeRFOSR, NeRFOSRDataParserConfig\n",
    "from nerfstudio.models.reni_neus import RENINeuSModel, RENINeuSModelConfig\n",
    "from nerfstudio.pipelines.base_pipeline import VanillaDataManager\n",
    "from nerfstudio.field_components.field_heads import FieldHeadNames\n",
    "from nerfstudio.fields.reni_field import get_directions\n",
    "from nerfstudio.fields.reni_field import RENIField, get_directions, get_sineweight\n",
    "from nerfstudio.cameras.rays import RayBundle\n",
    "\n",
    "def make_ray_bundle_copy(ray_bundle):\n",
    "    new_ray_bundle = RayBundle(\n",
    "      origins=ray_bundle.origins.detach().clone(),\n",
    "      directions=ray_bundle.directions.detach().clone(),\n",
    "      pixel_area=ray_bundle.pixel_area.detach().clone(),\n",
    "      directions_norm=ray_bundle.directions_norm.detach().clone(),\n",
    "      camera_indices=ray_bundle.camera_indices.detach().clone(),\n",
    "      nears=ray_bundle.nears.detach().clone() if ray_bundle.nears is not None else None,\n",
    "      fars=ray_bundle.fars.detach().clone() if ray_bundle.fars is not None else None,\n",
    "    )\n",
    "    return new_ray_bundle\n",
    "\n",
    "def make_batch_clone(batch):\n",
    "    new_batch = {}\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            new_batch[key] = value.detach().clone()\n",
    "        else:\n",
    "            new_batch[key] = value\n",
    "    return new_batch       \n",
    "\n",
    "def sRGB(imgs):\n",
    "    # if shape is not B, C, H, W, then add batch dimension\n",
    "    if len(imgs.shape) == 3:\n",
    "        imgs = imgs.unsqueeze(0)\n",
    "    q = torch.quantile(torch.quantile(torch.quantile(imgs, 0.98, dim=(1)), 0.98, dim=(1)), 0.98, dim=(1))\n",
    "    imgs = imgs / q.unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
    "    imgs = torch.clamp(imgs, 0.0, 1.0)\n",
    "    imgs = torch.where(\n",
    "        imgs <= 0.0031308,\n",
    "        12.92 * imgs,\n",
    "        1.055 * torch.pow(torch.abs(imgs), 1 / 2.4) - 0.055,\n",
    "    )\n",
    "    return imgs\n",
    "\n",
    "def rotation_matrix(axis, angle):\n",
    "    \"\"\"\n",
    "    Return 3D rotation matrix for rotating around the given axis by the given angle.\n",
    "    \"\"\"\n",
    "    axis = np.asarray(axis)\n",
    "    axis = axis / np.sqrt(np.dot(axis, axis))\n",
    "    a = np.cos(angle / 2.0)\n",
    "    b, c, d = -axis * np.sin(angle / 2.0)\n",
    "    aa, bb, cc, dd = a * a, b * b, c * c, d * d\n",
    "    bc, ad, ac, ab, bd, cd = b * c, a * d, a * c, a * b, b * d, c * d\n",
    "    return np.array([[aa + bb - cc - dd, 2 * (bc + ad), 2 * (bd - ac)],\n",
    "                     [2 * (bc - ad), aa + cc - bb - dd, 2 * (cd + ab)],\n",
    "                     [2 * (bd + ac), 2 * (cd - ab), aa + dd - bb - cc]])\n",
    "\n",
    "# setup config\n",
    "test_mode = 'val'\n",
    "world_size = 1\n",
    "local_rank = 0\n",
    "device = 'cuda:0'\n",
    "\n",
    "ckpt_path = 'outputs/data-NeRF-OSR-Data/RENI-NeuS/latest_with_rot_and_clip_illumination/'\n",
    "step = 100000\n",
    "\n",
    "ckpt = torch.load(ckpt_path + '/sdfstudio_models' + f'/step-{step:09d}.ckpt', map_location=device)\n",
    "model_dict = {}\n",
    "for key in ckpt['pipeline'].keys():\n",
    "    if key.startswith('_model.'):\n",
    "        model_dict[key[7:]] = ckpt['pipeline'][key]\n",
    "\n",
    "# load yaml checkpoint config\n",
    "config_path = Path(ckpt_path) / 'config.yml'\n",
    "config = yaml.load(config_path.open(), Loader=yaml.Loader)\n",
    "\n",
    "pipeline_config = config.pipeline\n",
    "pipeline_config.datamanager.dataparser.scene = 'lk2'\n",
    "pipeline_config.datamanager.dataparser.use_session_data = False\n",
    "\n",
    "# if illumination_sampler_random_rotation not in pipeline.config.model add it and set to false\n",
    "try:\n",
    "    pipeline_config.model.illumination_sampler_random_rotation\n",
    "except AttributeError:\n",
    "    pipeline_config.model.illumination_sampler_random_rotation = True\n",
    "try:\n",
    "    pipeline_config.model.illumination_sample_remove_lower_hemisphere\n",
    "except AttributeError:\n",
    "    pipeline_config.model.illumination_sample_remove_lower_hemisphere = True\n",
    "\n",
    "datamanager: VanillaDataManager = pipeline_config.datamanager.setup(\n",
    "    device=device, test_mode=test_mode, world_size=world_size, local_rank=local_rank, \n",
    ")\n",
    "datamanager.to(device)\n",
    "# includes num_eval_data as needed for reni latent code fitting.\n",
    "model = pipeline_config.model.setup(\n",
    "    scene_box=datamanager.train_dataset.scene_box,\n",
    "    num_train_data=len(datamanager.train_dataset),\n",
    "    num_eval_data=len(datamanager.eval_dataset),\n",
    "    metadata=datamanager.train_dataset.metadata,\n",
    "    world_size=world_size,\n",
    "    local_rank=local_rank,\n",
    "    eval_latent_optimisation_source=pipeline_config.eval_latent_optimisation_source,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval()\n",
    "\n",
    "image_idx_original, camera_ray_bundle_original, batch_original = datamanager.next_eval_image(1)\n",
    "\n",
    "True # printing to hide long cell output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reni_field = RENIField(pipeline_config.model.reni_path, num_latent_codes=1673, fixed_decoder=False)\n",
    "reni = reni_field.reni\n",
    "reni.fixed_decoder = True\n",
    "\n",
    "z = torch.load('checkpoints/reni_pretrained_weights/z_point_light.pt')\n",
    "z = z.repeat(1673, 1, 1)\n",
    "reni.mu.data = z\n",
    "\n",
    "W = 512\n",
    "H = W // 2\n",
    "D = get_directions(W)\n",
    "\n",
    "axis = [0, 1, 0]\n",
    "angle = np.pi / 1 # 45 degrees\n",
    "R = torch.tensor(rotation_matrix(axis, angle), dtype=torch.float32)\n",
    "reni.mu.data = torch.matmul(reni.mu.data, R)\n",
    "\n",
    "idx = 0\n",
    "img = reni(idx, D)\n",
    "img = img.reshape(H, W, 3)\n",
    "img = img.permute(2, 0, 1) # to CHW\n",
    "img = reni.unnormalise(img)\n",
    "img = sRGB(img)[0]\n",
    "plt.imshow(img.detach().cpu().numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Z = torch.load('checkpoints/reni_pretrained_weights/z_point_light.pt')\n",
    "Z = Z.repeat(model.num_eval_data, 1, 1).to(device)\n",
    "model.illumination_field_eval.reni.mu.data = Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Generating output for camera...</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\"> 12%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:03:41</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mGenerating output for camera...\u001b[0m \u001b[38;2;249;38;114m━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 12%\u001b[0m \u001b[36m0:03:41\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">sdf torch.Size([3, 606, 1, 1]) origins torch.Size([3, 606, 3]) ray_directions torch.Size([3, 606, 1, 3])\n",
       "</pre>\n"
      ],
      "text/plain": [
       "sdf torch.Size([3, 606, 1, 1]) origins torch.Size([3, 606, 3]) ray_directions torch.Size([3, 606, 1, 3])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (606) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m camera_ray_bundle\u001b[38;5;241m.\u001b[39mfars \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(camera_ray_bundle\u001b[38;5;241m.\u001b[39mdirections_norm) \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mscene_box\u001b[38;5;241m.\u001b[39mfar\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meval_num_rays_per_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_outputs_for_camera_ray_bundle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcamera_ray_bundle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/user/.local/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/code/sdfstudio/nerfstudio/models/base_model.py:195\u001b[0m, in \u001b[0;36mModel.get_outputs_for_camera_ray_bundle\u001b[0;34m(self, camera_ray_bundle, show_progress)\u001b[0m\n\u001b[1;32m    193\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m num_rays_per_chunk\n\u001b[1;32m    194\u001b[0m ray_bundle \u001b[38;5;241m=\u001b[39m camera_ray_bundle\u001b[38;5;241m.\u001b[39mget_row_major_sliced_ray_bundle(start_idx, end_idx)\n\u001b[0;32m--> 195\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mray_bundle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output_name, output \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/scratch/code/sdfstudio/nerfstudio/models/reni_neus.py:927\u001b[0m, in \u001b[0;36mRENINeuSModel.forward\u001b[0;34m(self, ray_bundle, step)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollider \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    925\u001b[0m     ray_bundle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollider(ray_bundle)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mray_bundle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/scratch/code/sdfstudio/nerfstudio/models/reni_neus.py:518\u001b[0m, in \u001b[0;36mRENINeuSModel.get_outputs\u001b[0;34m(self, ray_bundle, step)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, ray_bundle: RayBundle, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;66;03m# TODO make this configurable\u001b[39;00m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# compute near and far from from sphere with radius 1.0\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# ray_bundle = self.sphere_collider(ray_bundle)\u001b[39;00m\n\u001b[0;32m--> 518\u001b[0m     samples_and_field_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_and_forward_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mray_bundle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray_bundle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m# Shortcuts\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     field_outputs \u001b[38;5;241m=\u001b[39m samples_and_field_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/scratch/code/sdfstudio/nerfstudio/models/reni_neus.py:475\u001b[0m, in \u001b[0;36mRENINeuSModel.sample_and_forward_field\u001b[0;34m(self, ray_bundle, step)\u001b[0m\n\u001b[1;32m    472\u001b[0m sdf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(sdf, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4.0\u001b[39m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdf\u001b[39m\u001b[38;5;124m\"\u001b[39m, sdf\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morigins\u001b[39m\u001b[38;5;124m\"\u001b[39m, origins\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mray_directions\u001b[39m\u001b[38;5;124m\"\u001b[39m, ray_directions\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 475\u001b[0m origins \u001b[38;5;241m=\u001b[39m \u001b[43morigins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mray_directions\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# Sphere tracing loop\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;66;03m# Evaluate SDF at current position\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (606) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model.use_visibility = 'sphere_tracing'\n",
    "model.icosphere_order = 11\n",
    "camera_ray_bundle = make_ray_bundle_copy(camera_ray_bundle_original)\n",
    "batch = make_batch_clone(batch_original)\n",
    "\n",
    "camera_ray_bundle.nears = torch.zeros_like(camera_ray_bundle.directions_norm) + model.scene_box.near\n",
    "\n",
    "camera_ray_bundle.fars = torch.zeros_like(camera_ray_bundle.directions_norm) + model.scene_box.far\n",
    "\n",
    "model.config.eval_num_rays_per_chunk = 256\n",
    "\n",
    "outputs = model.get_outputs_for_camera_ray_bundle(camera_ray_bundle, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(outputs['rgb'].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "axis = [0, 1, 0]  # y-axis\n",
    "for angle in tqdm.tqdm(range(0, 721, 10)):\n",
    "    R = torch.tensor(rotation_matrix(axis, np.radians(angle)), dtype=torch.float32).to(device)\n",
    "    model.illumination_field_eval.reni.mu.data = torch.matmul(Z, R)\n",
    "    # get the current \n",
    "    camera_ray_bundle = make_ray_bundle_copy(camera_ray_bundle_original)\n",
    "    batch = make_batch_clone(batch_original)\n",
    "\n",
    "    camera_ray_bundle.nears = torch.zeros_like(camera_ray_bundle.directions_norm) + model.scene_box.near\n",
    "\n",
    "    camera_ray_bundle.fars = torch.zeros_like(camera_ray_bundle.directions_norm) + model.scene_box.far\n",
    "\n",
    "    model.config.eval_num_rays_per_chunk = 2048\n",
    "\n",
    "    outputs = model.get_outputs_for_camera_ray_bundle(camera_ray_bundle)\n",
    "\n",
    "    imgs.append(outputs['rgb'].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def animate_images(images):\n",
    "    \"\"\"\n",
    "    Create an animation of a sequence of images.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(images[0])\n",
    "\n",
    "    def update(frame):\n",
    "        im.set_data(images[frame])\n",
    "        return [im]\n",
    "\n",
    "    anim = FuncAnimation(fig, update, frames=len(images), interval=100)\n",
    "    plt.close(fig)\n",
    "    return anim\n",
    "\n",
    "anim.save('rotation.mp4', fps=30, extra_args=['-vcodec', 'libx264'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.use_visibility = 'sdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "axis = [0, 1, 0]  # y-axis\n",
    "for angle in tqdm.tqdm(range(0, 361, 10)):\n",
    "    R = torch.tensor(rotation_matrix(axis, np.radians(angle)), dtype=torch.float32).to(device)\n",
    "    model.illumination_field_eval.reni.mu.data = torch.matmul(Z, R)\n",
    "    # get the current \n",
    "    camera_ray_bundle = make_ray_bundle_copy(camera_ray_bundle_original)\n",
    "    batch = make_batch_clone(batch_original)\n",
    "\n",
    "    camera_ray_bundle.nears = torch.zeros_like(camera_ray_bundle.directions_norm) + model.scene_box.near\n",
    "\n",
    "    camera_ray_bundle.fars = torch.zeros_like(camera_ray_bundle.directions_norm) + model.scene_box.far\n",
    "\n",
    "    model.config.eval_num_rays_per_chunk = 256\n",
    "\n",
    "    outputs = model.get_outputs_for_camera_ray_bundle(camera_ray_bundle)\n",
    "\n",
    "    imgs.append(outputs['rgb'].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def delete_all_variables():\n",
    "    import gc\n",
    "    for name in list(globals().keys()):\n",
    "        if name not in {\"delete_all_variables\"}:\n",
    "            del globals()[name]\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "# Call the function to delete all variables\n",
    "delete_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
